Notes
This document is for informal notes about things I have realized, or should do. 
It will also contain results/comparisons of models as I add them.
1/10/2026
-It has just dawned on me that this credit default data set is
a binary classification task. As such, we need a method for assigning
the linear models best guess to 0 or 1. Per ESL, we would assign one column of Y
to each class and then take the argmax of the prediction. So, I need a function for
converting the data. 
-DONE Add functionality for training and test data seperation
-Add ridge regression and logisitc regression and compare to OLS
-OLS: Train error: 0.1998  -  Test error: 0.2083

1/12/2026
-Split off main file into a general file which will contain general funtions e.g. 
train/test split, data loading, data cleaning, etc
-Created methods folder to hold individual methods
-Created OLS, ridge, and logisitic files for their individual implementations

1/13
-Gonna try some feature engineering/basis expansions, but I don't anticipate the later to 
have too much effect given the nature of the data. 
-My first idea is that a good predictor of credit default would be a large balanced
relative to the amount paid each month
-First attempt at this train error: 0.1998 -> 0.2028 and test error: 0.2083 -> 0.208. 
I guess this is an improvement, but we can probably do better
-I will try the above on ridge. It had no effect. 
-TODO Lasso? 

1/14
-Got logistic regression working! I think I want to implement it again using pytorch
gradient descent since that will be important for future methods
-I am actually going to start with OLS using gradient descent
-Seems like I should just rewrite everything to use pytorch tensors
-Updated load_data, add_ones, MSE, convert_class, tt_split still works
-I think I am done with the refactor since all prior methods work correctly

1/18
-Implented logistic regression using gradient descent. It doesn't work as well
as the Newton updates
-Tried out quadratic basis expansion, but it sucks
-I want to implemented general additive models, but it seems I need to implement smoothing splines first
-See section 5.6 for B-splines, see page 151 for cubic splines, see section 9.1 for backfitting algorithm
-I am going to generate some artificial data to test the spline method on
-See appendix 186 for basis for splines

1/20
-Implementing B-splines is challenging, but I am getting there.
-Added email address to track git commits

1/23
-I have implemented the back fitting algorithm as described in ESL 9.1, however there are 
issues directly related to the data I have. Namely, you cannot fit a cubic spline onto
categorical data. I need to improve my knot generating function to better capture this 
reality. 
-I enjoying seeing where these methods, described in exact mathematical terms, conflict
with the realities of data. I think being able to more adeptly handle the conversion
from idea to implementation is one of the main reasons I started this project. 

1/24
-I've got, I think, two issues with the back fitting method at the moment. Firstly,
the knot selection as mentioned above seems to be leading to univertable matrices.
Secondly, my function to generate the B-splines on the data is just returning a list of
zeroes. The latter is more pressing, but I also have no idea why. I don't believe the length
of the data has anything to do with it based on checking against my toy example. 
-I think I have addressed the second issue by normalizing the data before fitting the spline
